---
created: 2025-12-18
tags:
  - 公众号文章
  - 工具调用
---

本文试图讲清楚一件事：**大模型是如何学会使用工具的**？

我们将聚焦于“工具调用”（Tool Calling）这一关键技术的发展脉络，尤其关注**每个阶段具体是怎么实现的**，帮助我和有需要的读者正理解这个越来越常见的 AI 能力。

---

## 什么是工具调用？
工具调用，指的是大模型在处理任务时，能够识别出任务的需求，并主动调用各种外部工具来获取信息、执行操作或者完成推理，再将信息整合进最终回答中。

简单来说，**工具调用就是大模型在回答问题时，发现自己“不知道”或“做不到”，于是主动请求外部帮手来完成任务。**

外部工具包括计算器、搜索引擎、代码解释器、数据库接口或者其它自定义API等。

有了能调用工具的能力，大模型不再局限于预训练和微调学到的静态知识，也有能力向智能体（Agent）的方向发展：一个能够自主感知环境、规划行动、调用工具并持续学习进步的AI工具。

在人工智能不断迈向实用化与自主化的进程中，工具调用已然成为大语言模型突破自身局限的关键能力。

---

## 一、发展起点：GPT3
2020年，大家现在耳熟能详的 ChatGPT 还没诞生，大语言模型（LLM）正处于早期探索阶段。

这一年，OpenAI 发布了 GPT-3，并向一小部分开发者开放了 API 接口。

很快，开发者们发现了一个有趣的现象：**只要在提示词（Prompt）里“告诉”模型它能调用某些工具，它就会在回答中主动“请求”使用这些工具。**

比如我向大模型提问：“今天天气怎么样？”
大模型会回复：“我需要调用天气查询API进行信息搜索。”

这看起来好像模型“知道”自己不能凭空回答天气问题，得靠外部工具帮忙——**这正是工具调用的雏形。**

---

### 技术实现
那么问题来了：**GPT-3 本身只是一个文本生成模型，它既不会思考，也不会真的调用程序，那它是怎么“学会”什么时候该调用工具、什么时候直接回答的呢？**

答案是：**靠“提示工程”（Prompt Engineering）+ 外部程序的规则匹配。**

简单来说，开发者会通过精心设计的“系统提示”（System Prompt），给模型设定一个角色和行为规则。比如，可以这样“教”模型（以下是一个模拟 2020 年的简化版提示）：

```text
你是一个智能助手，可以使用以下工具： 
- weather(city): 查询某城市的天气 

当用户的问题需要外部信息时，请严格按以下格式回复： 
{"tool": "weather", "params": {"city": "城市名"}} 

如果问题可以直接回答，请直接给出自然语言答案。
```

这样当用户询问：“今天北京天气怎么样”时，模型就会输出：

```json
{"tool": "weather", "params": {"city": "北京"}}
```

在模型按照规则进行输出后，**关键一步在于开发者使用外部程序进行判断**：

例如可以写：
	- 如果输出为合法json格式且包含“tool”字段→调用对应工具
- 否则→直接进行回答

这样，当外部程序检测到json输出后，就会去调用对应的工具，在拿到结果后将结果附上新的提示词喂给大模型，比如：

```text
工具返回结果：北京今天多云，无雨。

请用自然语言对用户进行回复。
```

最后，大模型就会回答正确的答案。

<font color="#245bdb">💡 打个比方：这时候的 GPT-3 就像一个只会“写纸条”的实习生——它知道自己不会查天气，但会写一张“请帮我查北京天气”的纸条。而真正的工作（调用 API、处理数据）全靠外面的主管（外部程序）来完成。</font>

---

### 局限性
尽管这种方法让模型看起来能“调用工具”，但它其实有很多明显的短板，尤其在面对复杂任务时：
1. 一次只能调一个工具
	比如问：“周杰伦生日那天北京天气如何？”
	
	这个问题其实需要两步：先查周杰伦的生日，再查那天北京的天气。但 GPT-3 不会自己拆解步骤，也无法连续调用两个工具——它只会按提示里写的单一格式输出一次，剩下的就卡住了。
	
2. 所有流程都得提前写死
	模型本身没有真正的判断力。工具怎么用、什么情况下用、用完之后下一步做什么……这些逻辑全靠开发者在提示词里硬编码。一旦遇到提示词没覆盖的新情况，模型就容易宕机。
3. 极度依赖用户输入完整参数
	如果用户问：“今天天气怎么样？”但没说城市，模型也不敢随便猜一个地方填进去。结果就是：任务失败，因为缺少必要信息。

---

### 一句话总结

> OpenAI 早期的函数调用 = 模型生成结构化指令（JSON）→ 外部程序识别并执行工具 → 返回结果给模型 → 模型回答用户

---

## 二、主动出击：WebGPT
GPT-3 虽然知识渊博，但它有个致命短板：**所有知识都来自训练数据，无法获取训练之后的新信息。**

所以当用户提出一些具有时效性的问题是，GPT-3就无能为力了。

为了解决这个问题，OpenAI 在 2021 年底推出了 WebGPT ——一个能让大模型“主动上网查资料”的新尝试。

它基于 GPT-3 微调而来，但最大的不同在于：**它不再被动等待指令，而是学会了像人一样主动使用网络浏览器去寻找答案。**

WebGPT在收到用户的问题后，可以像人类一样使用互联网搜索相关信息、生成带有引用链接的回复。目前市面上大部分AI应用网页端都已将联网搜索作为最基础的功能。

<font color="#245bdb">💡 打个比方：如果说 GPT-3 是一个只读过 2021 年以前所有书的学者，那 WebGPT 就是他第一次被允许走进图书馆，自己翻找最新期刊和网页来回答问题。</font>

---

### 技术实现

为什么WebGPT能够知道何时该调用工具、进行联网搜索，并且学会怎么搜索更有效？

靠的不是简单的提示词，而是一套更高级的训练方法：**基于人类反馈的强化学习（RLHF）**。

具体实现上可以分为两步：

1. 首先用监督学习（SFT)的方法对大模型进行微调：
	给模型提供大量的人工标注的“问答→浏览器操作序列”的训练数据，比如：
	
	```text
	Q：谁赢得了 2021 年图灵奖？ 
	A：
	search("2021 Turing Award winner")  // 搜索关键词
	click(0)  // 点第一个结果 
	quote("Jack Dongarra was awarded the 2021 ACM A.M. Turing Award.") //引用网页中的原话
	```

2. 再用RLHF进行训练：
	通过人工的方式对不同操作序列下生成的结果进行一系列指标评估并打分，用这些数据训练一个奖励模型，鼓励大模型向分数更高的生成方式靠齐，让它真正学会更高效准确的搜索方法。

久而久之，**模型就学会了优先选择更高效、更可靠的搜索策略**。

在实际对话过程中，模型的推理过程如下图所示，总结起来就是“搜索→点击→引用”。

![[Pasted image 20251217155822.png]]



---

### 局限性
尽管进步很大，WebGPT仍存在一定局限性：
1. 只能访问公开网页
	它的操作被限制在“只读”模式——比如不能登录你的邮箱、不能填表提交订单、也不能访问需要账号权限的内容（如付费墙后的文章）。
2. 依赖搜索引擎质量
	如果搜不到好结果，大模型也无能为力，可能还是会给出错误答案。
3. 计算成本提高
	一次问答中会涉及到多次模型调用和网页加载，需要更多的算力和网络资源。

正因如此，研究者意识到：**光让大模型会操作浏览器还不够，还得让它真正理解什么时候该调用工具、调哪个工具、以及如何组合多个工具。**

---

### 一句话总结

> WebGPT = 大模型 + 受限的浏览器操作能力 + 基于人类反馈的强化学习

---

## 三、思行合一：ReAct
WebGPT 虽然能主动上网查资料，但它有个明显短板：**查完就直接给答案，不会通过思考整合信息。**

比如面对一个需要多步推理的问题（如“从东京飞首尔要不要护照？”），它可能只会机械地执行一次搜索，而无法拆解问题、分步解决。

为了解决这个问题，2022 年，普林斯顿大学与谷歌的研究人员共同提出了**一种新范式：ReAct（Reasoning + Acting**），也成为了之后智能体开发的标准范式。

ReAct首次系统性的将思维链（Chain of Thought）与外部工具调用融合进一个大框架中，让模型不仅能想得明白，也能做得明白。

不同于早期纯推理或者纯行动的方法，ReAct在输出中交替生成“Thought（思考）”、“Action（行动）”和“Observation（观察）”步骤，构建了一个闭环的智能体工作流。

<font color="#245bdb">💡 打个比方：如果说 WebGPT 像一个只会按指令查资料的图书管理员，那 ReAct 就像一位侦探——它会先分析案情（思考），再决定去哪个档案室调取证据（行动），看到结果后又重新推理（观察 → 再思考），直到破案为止。</font>

---

### 技术实现

那么，ReAct是如何教会大模型这样思考的呢？

开发者采用的方法是**提示工程 + 少样本示例（Few-shot Prompting）**。

开发者会提供给大模型一个包含ReAct格式的提示模板，明确告诉大模型遇到问题时，应该先做哪一步、再做哪一步。

例如针对“从东京飞往首尔需要护照吗？”这个问题，提示模板会包含一个完整示例：

```python
## 你可以使用以下工具：

- check_flight_route(origin, dest)：返回该航班是国内航班还是国际航班。
- get_entry_requirements(country, citizenship)：返回前往该国家所需的护照/签证要求。

## 请使用以下格式：  

Thought: ...  
Action: 工具名称  
Action Input: { ... }  
Observation: ...  
...（根据需要重复上述步骤）  
Answer: ...

## 示例：  

Question：从东京飞往首尔需要护照吗？  
Thought: 我需要先确认这是否是一趟国际航班。  
Action: check_flight_route  
Action Input: {"origin": "Tokyo", "destination": "Seoul"}  
Observation: 国际航班。  
Thought: 那么我应该查询韩国的入境要求。  
Action: get_entry_requirements  
Action Input: {"country": "South Korea", "citizenship": "Japan"}  
Observation: 日本公民需要护照。  
Answer: 是的，你需要护照。
```

通过这样的示范教学，大模型就能在面对新问题时，**自动模仿这种“思考 → 行动 → 观察”的循环流程**，而不是一股脑地瞎猜或只做一步操作。

---

### 局限性

尽管 ReAct 极大地提升了模型处理复杂任务的能力，它仍然不是万能的：

1. 缺乏全局规划能力：
	ReAct采用的方法是走一步看一步，缺乏对长程任务的规划能力，容易在复杂任务中陷入局部最优甚至死循环；

2. 依赖高质量工具集：
	ReAct依赖预先定义好的工具名称和输入格式，如果工具集设置不够全面或工具返回出错，任务依然会失败。

正因如此，后续研究开始探索更高级的机制——比如让模型能自主规划任务步骤、动态组合工具，甚至自我反思失败原因。这些想法，最终催生了“原生工具调用”和现代智能体平台的发展。

---

### 一句话总结

> ReAct = “思考 → 行动 → 观察 → 再思考”的闭环工作流

---

## 四、原生支持：原生工具调用

ReAct 虽然聪明，但它本质上还是在演戏：大模型用自然语言假装自己在调用工具，比如输出一句：“我想调用 get_weather("北京")”。

但这种做法有个大问题：**只要格式稍有偏差（比如多一个空格、少个引号），外部程序就可能解析失败，整个流程就崩了**。

因此，为了能让大模型能结构化地请求调用工具，而不是靠文本模仿，各家AI公司纷纷让自家**模型原生支持工具调用**。

例如，2023 年 6 月，OpenAI 在其Chat Completions API中正式新增Function Calling功能；2024年，Claude 3.5也支持原生工具调用等等。下面将以OpenAI为例进行说明。

在支持原生工具调用之后，大模型不再通过文本的方式去说出要调哪个工具，而是直接通过 API 响应中的专用字段，返回一个结构化的函数调用请求。

这意味着：模型不再通过文本表述要调什么工具，而是直接告诉系统要调哪个函数，用什么参数，而且是**以机器能 100% 看懂的方式**。

<font color="#245bdb">💡 打个比方：以前的工具调用像是员工手写一张“请帮我查天气”的便签递给 IT 部门；而原生工具调用则像是在公司内部系统里直接点击“天气查询”按钮——系统自动识别、自动执行，零误差。</font>

---

### 技术实现

我们用一个具体例子来看看 OpenAI 的原生工具调用是怎么工作的。

首先，开发者需要提前用标准 JSON 格式定义好可用工具。比如：

```json
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "获取指定城市的当前天气",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {"type": "string", "description": "城市名称，如 '北京'"},
                },
                "required": ["city"]
            }
        }
    }
]
```

这个定义就像一份“工具说明书”，明确告诉模型：“你只能调用 get_weather 这个函数，而且必须提供 city 参数。”

当用户进行提问：“今天北京天气怎么样？”

平台会把问题 + 工具定义一起传给大模型 API：

```json
{
  "model": "gpt-3.5-turbo",
  "messages": [{"role": "user", "content": "今天北京天气怎么样？"}],
  "tools": tools,               // ← 把上面的工具定义传进去
  "tool_choice": "auto"         // 让模型自己决定是否调用
}
```

这一步是关键：**模型不再输出一段话，而是通过专用字段 tool_calls 返回一个机器可读的调用请求**：

```json
{
  "message": {
    "role": "assistant",
    "content": null, // ← 不再生成自然语言回答
    "tool_calls": [
      {
        "id": "call_abc123",
        "type": "function",
        "function": {
          "name": "get_weather",
          "arguments": "{\"city\": \"北京\"}" // ← 严格符合预定义格式
        }
      }
    ]
  }
}
```

后端程序收到这个结构化请求后：

- 自动校验工具名和参数是否合法；
- 调用真实函数（如 get_weather("北京")）；
- 拿到结果（比如 "多云，18°C"）后，以标准格式回传给模型：

```python
weather_result = get_weather("北京")  # 返回: "多云，18°C"
```
```json
{
  "role": "tool",
  "tool_call_id": "call_abc123",   // 匹配相同的 id
  "content": "多云，18°C"
}
```

最后，平台把整个对话历史（用户问题 + 工具调用 + 工具结果）再发给模型，让它生成人类能看懂的回答：

> “今天北京是多云天气，气温 18°C。”

从上面例子可以看出，原生工具调用大大提升了稳定性：
1. 不再依赖正则表达式或 JSON 解析器去猜模型想调什么工具,所有调用都是结构化数据，系统直接读取，出错率接近于零;
2. 工具名称、参数、类型，这些都由 API 后端自动检查，开发者不用再写一堆容错逻辑。
3. 由于底层技术的改变，现在采用的是受限解码（Constrained Decoding）的技术，大模型**只能从你提供的工具列表中选择函数名**，出现幻觉可能性大大降低。

---

### 一句话总结
> OpenAI Function Calling = 原生、可靠、结构化的工具调用机制

---

## 五、高效编排：LangChain

原生工具调用解决了“模型怎么可靠地请求工具”的问题，但对开发者来说，还有一个更现实的挑战：

> 如何快速把多个工具、记忆、推理逻辑拼在一起，做出一个能干活的 AI 智能体？

这时候，LangChain 这样的开源框架就登场了。

如果说原生工具调用是汽车的“发动机”，那 LangChain 就是整套“造车平台”——它把引擎、方向盘、轮胎、导航系统都标准化、模块化，开发者不用从零造轮子，只需按需组装，就能快速造出一辆能跑的车。

<font color="#245bdb">💡 打个比方：ReAct 提出了“智能体应该边想边做”的设计图,而 LangChain 把这张图纸变成了一套带说明书的乐高积木：你只要选好零件（工具、记忆、策略），动手一拼，就能得到属于你的智能体助手。</font>

---

### 技术实现

LangChain把工具调用拆分为了四个关键角色：

| 模块                 | 作用                          | 类比               |
| ------------------ | --------------------------- | ---------------- |
| Tool（工具）           | 封装外部功能，比如查天气、发邮件、读数据库       | “手”——执行具体任务      |
| Agent（智能体）         | 决定什么时候该用哪个工具，怎么思考问题         | “大脑”——负责决策与推理    |
| AgentExecutor（执行器） | 管理“思考 → 调用 → 观察 → 再思考”的完整循环 | “调度员”——协调大脑和手的配合 |
| Memory（记忆）         | 记住对话历史或长期状态                 | “笔记本”——记录过往信息    |

我们还是举一个例子：

> 用户的问题是：“我上周提到的项目进展如何？顺便查一下北京明天的天气。”

现在开始使用LangChain配置工具：

```python
from langchain.tools import tool

@tool
def get_weather(city: str) -> str:
    # 调用真实天气 API
    return 真实天气情况
```

选择Agent类型：

```python
from langchain.agents import create_tool_calling_agent
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4-turbo")
agent = create_tool_calling_agent(llm, tools=[get_weather], prompt=react_prompt) # 最常用的ReAct Agent
```

配置Memory：

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)
```

最后，运行AgentExecutor：

```python
from langchain.agents import AgentExecutor

executor = AgentExecutor(
    agent=agent,
    tools=[get_weather],
    memory=memory,
    verbose=True  # 打印每一步 Thought/Action
)

response = executor.invoke({"input": "我上周提到的项目进展如何？顺便查一下北京明天的天气。"})
```

当`executor.invoke()`被调用时，LangChain会自动进行“推理+工具调用+整合记忆+工具结果”。

从上面这个例子可以看出，开发者只需关注“我要什么工具”“用什么记忆”“选哪种 Agent”，其余繁琐的循环控制、格式校验、错误处理，全部由 LangChain 自动搞定。

---

### 一句话总结
> LangChain = 模块化功能 + 自动调度系统

---

## 六、标准协议：MCP

随着大模型原生工具调用能力的普及和 LangChain 等开发框架的成熟，AI 智能体的构建门槛显著降低，各类智能体平台也如雨后春笋般涌现。然而，一个深层次的瓶颈随之浮现：**工具生态越繁荣，接口标准却越混乱**。

每个平台、每个模型都要为同一个工具（比如查天气、读 PDF）单独写一套对接逻辑，开发者疲于“重复造轮子”。更麻烦的是，换一个模型，就得重写一遍集成代码——效率低、维护难、体验割裂。

为了解决这一痛点，2024 年底，Anthropic 提出了 **MCP（Model Context Protocol）** ——一个专为大模型与外部工具设计的**统一通信协议**。

MCP 并不是一种新的工具调用方式，而更像是 **AI 时代的 USB 接口**：它不关心你插的是键盘还是U盘，只提供一个标准化的插槽。只要符合协议，任何工具都能被任何支持 MCP 的模型或平台即插即用。

<font color="#245bdb">💡 打个比方：过去每台电脑都要为鼠标、打印机、硬盘分别安装专用驱动；如今有了 USB 标准，外设一插即用。MCP 正是要为 AI 工具生态装上这样一个“通用插槽”，让智能体真正摆脱接口碎片化的枷锁。</font>

---

### 技术实现

MCP 基于经典的客户端-服务器架构，但对角色做了清晰划分：

- **MCP Host**：运行大模型的应用程序（如 Cursor、Cherry Studio、Claude Desktop），负责接收用户请求，并发起工具调用；

- **MCP Client**：嵌入在 Host 中的通信模块，将模型的意图（比如“我想读这份PDF”）转换成标准的 MCP 请求；

- **MCP Server**：提供具体能力的服务端，可部署在本地或云端，用于注册一组 **Tools（工具）**、**Resources（资源）** 或 Prompts。


![[file-20260104-151452-626.jpg]]

需要特别强调的是：**MCP 不是 Function Calling 的替代品，而是它的“标准化外壳”**。

- 大模型依然通过原生 Function Calling告诉系统：“我要调用哪个工具、传什么参数”；
- Agent 负责执行调用逻辑；
- 而 MCP 则定义了工具应该如何被描述、发现和安全调用。

这样一来，开发者只需实现一次 MCP Server（例如一个本地 PDF 解析服务），就能被所有支持 MCP 的 Host（无论背后是 GPT、Claude 还是其他模型）直接调用——真正实现 “一次开发，处处可用”。

更关键的是，MCP 内置了**安全隔离机制**：

- 敏感凭证（如 API Key、数据库密码）永远只保存在 Server 端，模型无法直接接触；
- Host 与 Server 之间的通信支持多种方式：本地通过 Stdio，远程通过 SSE 或 HTTP；
- 同时支持细粒度的能力声明（比如“只允许读取指定目录”）和调用行为审计，大幅提升安全性与可控性。

Agent调用工具的效果受到诸多因素制约——工具选择能力、任务规划能力、上下文理解能力——而这些能力的培养与提升，都不在MCP的职责范围内。MCP只承诺提供统一的工具接口，而不承诺这些工具将被如何选择和组合。

---

### 一句话总结

> MCP = AI 时代的USB接口

---

## 七、平台时代：智能体平台

在 MCP 这类统一协议的推动下，AI 智能体的能力边界迅速扩展：

- 从单个问题回答，变成多个 AI 角色协作完成任务；
- 从调用固定工具，变成在动态环境中实时感知与响应；
- 从一次对话就结束，变成长期记忆、持续学习、越用越聪明；

这时候，光靠 LangChain 这样的代码框架已经不够用了。

开发者不仅要编写业务逻辑，还需自行处理**多智能体调度、上下文记忆管理、安全权限控制、行为评估、版本回滚**等一整套基础设施问题，工作量堪比自己造操作系统。

如今2025 年，行业重心已经从“如何构建一个智能体”，转向“**如何运行一个智能体生态**”。

大量的智能体平台开始涌现。它们不再只是开发库，而是变成**一体化操作系统**，集成了：

- 工具市场（现成工具一键复用）
- 记忆中枢（长期存储用户偏好与历史）
- MCP（统一调度多个模型与工具）
- 安全沙箱（防止 AI 胡乱调用敏感接口）
- 自动评估与迭代机制（根据用户反馈自动优化行为）


常见的平台包括：OpenAI Assistants API、LangChain + LangGraph、LlamaIndex、Dify、Coze 等。

<font color="#245bdb">💡 打个比方：如果说 LangChain 是让你亲手搭电路，还得自己接线（写代码）；那 Dify 就像是给你一个预装好各种组件的智能控制面板。你不需要碰底层线路，只要在图形界面上把“按开关→ 触发照明模块”这样的逻辑连起来，它就能自动运行。</font>

---

### 技术实现

如今的智能体平台，把工具调用这件事变得**极其简单**，甚至**无需编程基础**。开发者所有的操作都可以在前端界面通过拖拉拽和低代码的方式实现，大量的调度、校验、执行都由平台自动完成。

当前像 dify 这类支持“拖拉拽搭建智能体”的低代码/零代码平台，其底层的工具调用能力并非单一技术，而是融合了多种成熟与新兴技术栈的系统工程。

 1. 基础层：大模型原生 Function Calling 能力

- 平台优先接入支持原生工具调用的大模型（如 GPT-4、Claude 3.5），使其在推理时直接输出结构化的函数调用请求，避免依赖正则匹配或提示词“猜”格式。
- 对于不支持原生调用的模型，则回退至 **ReAct 模式**，通过精心设计的 Prompt 引导模型按 `Thought → Action → Observation` 的格式生成工具调用指令。

 2. 中间层：平台级插件系统 + 工作流引擎

- 所有工具——无论是内置函数、HTTP 接口，还是 Python 脚本——在平台中均需注册为标准化的 **Tool 对象**，包含以下元信息：
    - 名称与描述；
    - 参数 Schema（类型、是否必填等）；
    - 执行逻辑（Python 代码 / HTTP URL / MCP Server 地址）。
- 运行时，平台通过 **ToolManager 路由器** 定位具体实现，并在**安全沙箱**中执行，有效隔离潜在风险。

```json
{
  "name": "weather_api",
  "description": "获取指定城市天气",
  "parameters": {
    "type": "object",
    "properties": { "city": { "type": "string" } },
    "required": ["city"]
  }
}
```

用户在界面勾选所需插件后，平台（如 Coze）会自动将其注册到所选大模型的工具列表中。  
同时，平台提供**图形化流程编排器**，由工作流引擎按拓扑顺序执行节点，并在各步骤间传递上下文数据，实现复杂任务的自动化串联。

3. 连接层：MCP

- 用户只需配置 MCP Server 地址（如 `http://localhost:9000/sse`），即可将外部服务（如 FastMCP 编写的爬虫、高德地图接口）一键接入智能体。
- Dify 内置两类 MCP 插件：
    - `mcp_sse_list_tools`：拉取远程 Server 提供的可用工具列表；
    - `mcp_sse_call_tool`：发起具体工具调用。


```json
{
  "my_search": {
    "url": "http://127.0.0.1:8000/sse",
    "transport": "sse",
    "timeout": 60
  }
}
```

此外，平台还通过**异步调用 + 任务队列 + 重试机制**，为耗时操作提供可靠执行保障。例如，针对视频处理、大文件分析等长周期任务，Dify 支持：

- 标记 `execute_async: true`，立即返回 `task_id`；
- 后台通过 **Celery + Redis 队列** 执行任务；
- 可配置最大重试次数、指数退避策略与超时窗口；
- 客户端通过轮询 `/v1/tasks/{id}` 获取最终结果。

---

### 一句话总结

> 智能体平台 = 可视化拖拉拽 + 低代码配置 + 企业级运行环境

---

## 结语


从 GPT-3 时代依赖提示工程的“伪调用”，到 WebGPT 主动联网搜索的初步探索；

从 ReAct 构建“思考—行动—观察”的智能闭环，到原生工具调用实现结构化、高可靠性的函数交互；

再到 LangChain 等框架将智能体开发模块化、标准化，继而由 MCP推动工具接口的统一与互操作；

直至今日，各类智能体平台以低代码、可视化的方式集成记忆、安全、评估与协作能力，真正让智能体开发走向大众。

在这条路径上，大模型一步步从被动回答者，成长为主动执行者、协作者，甚至自主决策者。

未来，随着工具生态的丰富、记忆机制的深化以及多智能体协同架构的成熟，我们或将见证真正具备环境感知、目标规划与自我迭代能力的通用人工智能（AGI）诞生。

而工具调用，正是这条通往 AGI 之路上不可或缺的桥梁。
